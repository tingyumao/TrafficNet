{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tingyumao/Programs/anaconda3/envs/dlenv/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from ssd.ssd import SSD, Detector, UpSample, DownSample\n",
    "from ssd.ssd_loss import MultiboxLoss\n",
    "from ssd.ssd_bbox import BBoxUtility\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "# set_session(tf.Session(config=config))\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some constants\n",
    "NUM_CLASSES = 4 # remember the background \n",
    "input_shape = (384, 384, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7308, 8)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "## priorbox defined by ourselves\n",
    "prior_tensor = tf.ones((1, 300, 300, 3))\n",
    "ssd_net = SSD(input_shape, NUM_CLASSES)\n",
    "_, priors = ssd_net(prior_tensor, PRIORS = True)\n",
    "print(priors.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 11508, 8)\n"
     ]
    }
   ],
   "source": [
    "# DEBUG by tensorboard\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #a = tf.placeholder(tf.float32, name=\"a\")\n",
    "    #b = tf.placeholder(tf.float32, name=\"b\")\n",
    "    #c = a + b\n",
    "    ## priorbox defined by ourselves\n",
    "    input_tensor = tf.ones((1, 384, 384, 3))\n",
    "    ssd_net = SSD()\n",
    "    feats = ssd_net(input_tensor)\n",
    "    detect_net = Detector(input_shape, NUM_CLASSES)\n",
    "    priors = detect_net(feats, PRIORS=True)\n",
    "    print(priors.shape)\n",
    "    \n",
    "tf.summary.FileWriter(\"logs\", g).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        priors_np = sess.run(priors)\n",
    "\n",
    "# save priors tensor\n",
    "with open('./data/priors384.pkl', 'wb') as handle:\n",
    "    pickle.dump(np.squeeze(priors_np), handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11508, 8)\n"
     ]
    }
   ],
   "source": [
    "# load priors tensor\n",
    "with open('./data/priors384.pkl', 'rb') as handle:\n",
    "    priors_np = pickle.load(handle)\n",
    "    print(priors_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess = tf.InteractiveSession()\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "#priors_np = np.squeeze(sess.run(priors))\n",
    "bbox_util = BBoxUtility(NUM_CLASSES, priors_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 11508, 8]\n"
     ]
    }
   ],
   "source": [
    "print([None,] + list(priors_np.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2DLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length = 4\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### final end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tingyumao/Programs/anaconda3/envs/dlenv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    seq_input_ph = tf.placeholder(tf.float32, [None, seq_length] + list(input_shape), name=\"input_image\")\n",
    "    gt_ph = tf.placeholder(tf.float32, [None, priors_np.shape[0], 16], name=\"ground_truth\")\n",
    "\n",
    "    with tf.variable_scope(\"ssd_net\"):\n",
    "        ssd_net = SSD()\n",
    "        ssd_input = tf.reshape(seq_input_ph, [-1,]+list(input_shape), name=\"reshape_seq_input\")\n",
    "        feats = ssd_net(ssd_input)\n",
    "        #seq_feats = tf.reshape(ssd_input, [-1, seq_length]+list(input_shape), name=\"reshape_seq\")\n",
    "\n",
    "    #with tf.variable_scope(\"up_net\"):\n",
    "    #    upsample_net = UpSample()\n",
    "    #    feats_up, up_factors = upsample_net(feats)\n",
    "    #    seq_feats_up = tf.reshape(ssd_input, [-1, seq_length]+feats_up.get_shape().as_list()[-3:], name=\"reshape_seq_up\")\n",
    "\n",
    "    with tf.variable_scope(\"core_conv_lstm\"):\n",
    "        pred_feats = []\n",
    "        for i, feat in enumerate(feats[:-1]):\n",
    "            seq_feat = tf.reshape(feat, [-1, seq_length]+feat.get_shape().as_list()[-3:], name=\"reshape_seq_{}\".format(i))\n",
    "            cell = tf.contrib.rnn.Conv2DLSTMCell(input_shape=feat.get_shape().as_list()[-3:], \n",
    "                                                 output_channels=128, kernel_shape=[5,5], name=\"convrnncell_{}\".format(i))\n",
    "            init_state = cell.zero_state(batch_size, tf.float32)\n",
    "            outputs, final_state = tf.nn.dynamic_rnn(cell, seq_feat, dtype=tf.float32, scope=\"rnn_{}\".format(i))\n",
    "            pred_feats.append(final_state[-1])\n",
    "            \n",
    "        # last feature vector\n",
    "        seq_feat = tf.reshape(feats[-1], [-1, seq_length]+feats[-1].get_shape().as_list()[-1:], name=\"last_reshape_seq\")\n",
    "        cell = tf.contrib.rnn.LSTMCell(128)\n",
    "        init_state = cell.zero_state(batch_size, tf.float32)\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, seq_feat, dtype=tf.float32, scope=\"last_dense_rnn\")\n",
    "        pred_feats.append(final_state[-1])\n",
    "        \n",
    "    #with tf.variable_scope(\"down_pooling\"):\n",
    "    #    down_net = DownSample()\n",
    "    #    feats_down = down_net(final_state[-1], up_factors) # final_state: (c, h)\n",
    "        \n",
    "    with tf.variable_scope(\"final_prediction\"):\n",
    "        detector = Detector(input_shape, NUM_CLASSES)\n",
    "        #predictions = detector(feats, PRIORS=False)\n",
    "        final_predictions = detector(pred_feats, PRIORS=False)\n",
    "        \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        loss = MultiboxLoss(NUM_CLASSES, neg_pos_ratio=2.0).compute_loss(gt_ph, final_predictions)\n",
    "    \n",
    "    with tf.variable_scope(\"train\"):\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        lr = tf.train.exponential_decay(3e-4, global_step, num_train//batch_size, 0.9, staircase=True, name=\"lr\")\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.summary.FileWriter(\"logs\", g).close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load annotations\n",
    "with open('./data/0254.pkl', 'rb') as handle:\n",
    "    annotation0254 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "## bounding box ground truth\n",
    "gt = dict(pickle.load(open('./data/0254.pkl', 'rb')))\n",
    "keys = list(gt.keys())\n",
    "num_train = int(round(0.8 * len(keys)))\n",
    "train_keys = keys[:num_train]\n",
    "val_keys = keys[num_train:]\n",
    "num_val = len(val_keys)\n",
    "print(num_train)\n",
    "print(num_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ssd.preprocess import *\n",
    "path_prefix = './data/0254img/'\n",
    "\n",
    "# batch_size = 4\n",
    "gen = SeqGenerator(gt, bbox_util, batch_size, path_prefix,\n",
    "                train_keys, val_keys, input_shape, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator = gen.generate(True)\n",
    "val_generator = gen.generate(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tingyumao/Programs/anaconda3/envs/dlenv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#g = tf.Graph()\n",
    "#with g.as_default():\n",
    "seq_input_ph = tf.placeholder(tf.float32, [None, seq_length] + list(input_shape), name=\"input_image\")\n",
    "gt_ph = tf.placeholder(tf.float32, [None, priors_np.shape[0], 16], name=\"ground_truth\")\n",
    "\n",
    "with tf.variable_scope(\"ssd_net\"):\n",
    "    ssd_net = SSD()\n",
    "    ssd_input = tf.reshape(seq_input_ph, [-1,]+list(input_shape), name=\"reshape_seq_input\")\n",
    "    feats = ssd_net(ssd_input)\n",
    "    #seq_feats = tf.reshape(ssd_input, [-1, seq_length]+list(input_shape), name=\"reshape_seq\")\n",
    "\n",
    "#with tf.variable_scope(\"up_net\"):\n",
    "#    upsample_net = UpSample()\n",
    "#    feats_up, up_factors = upsample_net(feats)\n",
    "#    seq_feats_up = tf.reshape(ssd_input, [-1, seq_length]+feats_up.get_shape().as_list()[-3:], name=\"reshape_seq_up\")\n",
    "\n",
    "with tf.variable_scope(\"core_conv_lstm\"):\n",
    "    pred_feats = []\n",
    "    for i, feat in enumerate(feats[:-1]):\n",
    "        seq_feat = tf.reshape(feat, [-1, seq_length]+feat.get_shape().as_list()[-3:], name=\"reshape_seq_{}\".format(i))\n",
    "        cell = tf.contrib.rnn.Conv2DLSTMCell(input_shape=feat.get_shape().as_list()[-3:], \n",
    "                                             output_channels=128, kernel_shape=[5,5], name=\"convrnncell_{}\".format(i))\n",
    "        init_state = cell.zero_state(batch_size, tf.float32)\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, seq_feat, dtype=tf.float32, scope=\"rnn_{}\".format(i))\n",
    "        pred_feats.append(final_state[-1])\n",
    "\n",
    "    # last feature vector\n",
    "    seq_feat = tf.reshape(feats[-1], [-1, seq_length]+feats[-1].get_shape().as_list()[-1:], name=\"last_reshape_seq\")\n",
    "    cell = tf.contrib.rnn.LSTMCell(128)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, seq_feat, dtype=tf.float32, scope=\"last_dense_rnn\")\n",
    "    pred_feats.append(final_state[-1])\n",
    "\n",
    "#with tf.variable_scope(\"down_pooling\"):\n",
    "#    down_net = DownSample()\n",
    "#    feats_down = down_net(final_state[-1], up_factors) # final_state: (c, h)\n",
    "\n",
    "with tf.variable_scope(\"final_prediction\"):\n",
    "    detector = Detector(input_shape, NUM_CLASSES)\n",
    "    #predictions = detector(feats, PRIORS=False)\n",
    "    final_predictions = detector(pred_feats, PRIORS=False)\n",
    "\n",
    "with tf.variable_scope(\"loss\"):\n",
    "    loss = MultiboxLoss(NUM_CLASSES, neg_pos_ratio=2.0).compute_loss(gt_ph, final_predictions)\n",
    "\n",
    "with tf.variable_scope(\"train\"):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    lr = tf.train.exponential_decay(3e-4, global_step, num_train//batch_size, 0.9, staircase=True, name=\"lr\")\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 20\n",
    "for e in range(nb_epoch):\n",
    "    # training phase\n",
    "    cnt = 0\n",
    "    for sample in train_generator:\n",
    "        train_input, train_target = sample\n",
    "        train_loss, _ = sess.run([loss, train_op], feed_dict={seq_input_ph: train_input, gt_ph: train_target})\n",
    "        train_loss_hist.append(np.mean(train_loss))\n",
    "        cnt += 1\n",
    "        print(\"training {}/{}, minibatch loss: {}\".format(cnt*batch_size, num_train, train_loss))\n",
    "        if cnt > (num_train//batch_size):\n",
    "            cnt = 0\n",
    "            break\n",
    "        \n",
    "    # validation phase\n",
    "    val_loss = 0.0\n",
    "    for sample in val_generator:\n",
    "        _, val_input, val_target = sample\n",
    "        val_loss += np.sum(sess.run(loss, feed_dict={seq_input_ph: val_input, gt_ph: val_target}))\n",
    "        cnt += 1\n",
    "        if cnt > (num_val//batch_size):\n",
    "            cnt = 0\n",
    "            break\n",
    "    \n",
    "    # print progress\n",
    "    verbose_loss = np.mean(train_loss_hist[-(num_train//batch_size+1):])\n",
    "    verbose_val_loss = val_loss/((num_val//batch_size+1)*batch_size)\n",
    "    print(\"epoch {}/{}, train loss: {}, val loss: {}\".format(e+1, nb_epoch, verbose_loss, verbose_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_image:0' shape=(?, 4, 384, 384, 3) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_input_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ground_truth:0' shape=(?, 11508, 16) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
